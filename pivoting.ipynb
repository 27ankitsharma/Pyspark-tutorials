{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Pivoting Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 20 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will illustrate how to reshape (pivot) data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few initial setup items:  First we test that the spark context was successfully created during bootstrap and is available in the global namespace as 'sc'.  After that we create the SQL context necessary for working with a dataframe (panel data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a toy dataframe to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row('state', 'industry', 'hq', 'jobs')\n",
    "\n",
    "df = sc.parallelize([\n",
    "    row('MI', 'auto', 'domestic', 716),\n",
    "    row('MI', 'auto', 'foreign', 123),\n",
    "    row('MI', 'auto', 'domestic', 1340),\n",
    "    row('MI', 'retail', 'foreign', 12),\n",
    "    row('MI', 'retail', 'foreign', 33),\n",
    "    row('OH', 'auto', 'domestic', 349),\n",
    "    row('OH', 'auto', 'foreign', 101),\n",
    "    row('OH', 'auto', 'foreign', 77),\n",
    "    row('OH', 'retail', 'domestic', 45),\n",
    "    row('OH', 'retail', 'foreign', 12)\n",
    "    ]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----+\n",
      "|state|industry|      hq|jobs|\n",
      "+-----+--------+--------+----+\n",
      "|   MI|    auto|domestic| 716|\n",
      "|   MI|    auto| foreign| 123|\n",
      "|   MI|    auto|domestic|1340|\n",
      "|   MI|  retail| foreign|  12|\n",
      "|   MI|  retail| foreign|  33|\n",
      "|   OH|    auto|domestic| 349|\n",
      "|   OH|    auto| foreign| 101|\n",
      "|   OH|    auto| foreign|  77|\n",
      "|   OH|  retail|domestic|  45|\n",
      "|   OH|  retail| foreign|  12|\n",
      "+-----+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot operations must always be preceeded by a groupBy operation.  In our first case we will simply pivot to show domestic versus foreign jobs in each of our two states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot1 = df.groupby('state').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+\n",
      "|state|domestic|foreign|\n",
      "+-----+--------+-------+\n",
      "|   OH|     394|    190|\n",
      "|   MI|    2056|    168|\n",
      "+-----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `values=['domestic', 'foreign']` part of the pivot method is optional.  If we don't supply a list then pySpark will attempt to infer the values, but naturally that requires more processing than if we specify it up front.  As your datasets get larger and larger this sort of help becomes more and more important.\n",
    "\n",
    "Here's another example, this time pivoting by both `state` and by `industry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot = df.groupBy('state', 'industry').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+-------+\n",
      "|state|industry|domestic|foreign|\n",
      "+-----+--------+--------+-------+\n",
      "|   MI|    auto|    2056|    123|\n",
      "|   OH|  retail|      45|     12|\n",
      "|   OH|    auto|     349|    178|\n",
      "|   MI|  retail|    null|     45|\n",
      "+-----+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sum` method at the end can be replaced as necessary, for example with `avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Pivoting Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 2 Aug 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will illustrate how to pivot data._\n",
    "\n",
    "_Main operations used: groupBy, pivot, sum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping\n",
    "\n",
    "Pivoting involves an aggregation, as we will see below.  If what you're looking for is reshaping, where a dataset is turned from wide to long or vice versa without the loss of any information, then that is not currently implemented in Spark.  The likely reason for this is that it's an incredibly costly operation; if you've performed it in Stata or SAS, for example, it probably took a while to compute even on smaller datasets.  Doing it on very large data that is distributed across many nodes is even worse.\n",
    " \n",
    "This may be something we see Spark implement with relative efficiency down the road, but for now the only way to accomplish it is with a custom looping structure that rebuilds the dataframe the way you want it.  In general however, if your data is very large, it's best to think about ways to avoid having to completely restructure it.\n",
    "\n",
    "As we will show at the end however, it is possible to make `pivot` work similarly to reshaping in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivoting\n",
    "\n",
    "To illustrate how pivoting works, we create a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row('state', 'industry', 'hq', 'jobs')\n",
    "\n",
    "df = sc.parallelize([\n",
    "    row('MI', 'auto', 'domestic', 716),\n",
    "    row('MI', 'auto', 'foreign', 123),\n",
    "    row('MI', 'auto', 'domestic', 1340),\n",
    "    row('MI', 'retail', 'foreign', 12),\n",
    "    row('MI', 'retail', 'foreign', 33),\n",
    "    row('OH', 'auto', 'domestic', 349),\n",
    "    row('OH', 'auto', 'foreign', 101),\n",
    "    row('OH', 'auto', 'foreign', 77),\n",
    "    row('OH', 'retail', 'domestic', 45),\n",
    "    row('OH', 'retail', 'foreign', 12)\n",
    "    ]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----+\n",
      "|state|industry|      hq|jobs|\n",
      "+-----+--------+--------+----+\n",
      "|   MI|    auto|domestic| 716|\n",
      "|   MI|    auto| foreign| 123|\n",
      "|   MI|    auto|domestic|1340|\n",
      "|   MI|  retail| foreign|  12|\n",
      "|   MI|  retail| foreign|  33|\n",
      "|   OH|    auto|domestic| 349|\n",
      "|   OH|    auto| foreign| 101|\n",
      "|   OH|    auto| foreign|  77|\n",
      "|   OH|  retail|domestic|  45|\n",
      "|   OH|  retail| foreign|  12|\n",
      "+-----+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot operations must always be preceeded by a groupBy operation.  In our first case we will simply pivot to show domestic versus foreign jobs in each of our two states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot1 = df.groupby('state').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+\n",
      "|state|domestic|foreign|\n",
      "+-----+--------+-------+\n",
      "|   OH|     394|    190|\n",
      "|   MI|    2056|    168|\n",
      "+-----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the `values=['domestic', 'foreign']` part of the pivot method is optional.**  If we don't supply a list then pySpark will attempt to infer the values, but naturally that requires more processing than if we specify it up front.  As your datasets get larger and larger this sort of help becomes more and more important.\n",
    "\n",
    "Here's another example, this time pivoting by both `state` and by `industry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot = df.groupBy('state', 'industry').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+-------+\n",
      "|state|industry|domestic|foreign|\n",
      "+-----+--------+--------+-------+\n",
      "|   MI|    auto|    2056|    123|\n",
      "|   OH|  retail|      45|     12|\n",
      "|   OH|    auto|     349|    178|\n",
      "|   MI|  retail|    null|     45|\n",
      "+-----+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sum` method at the end can be replaced as necessary, for example with `avg`.\n",
    "\n",
    "# Approximating Reshape\n",
    "\n",
    "Pivot requires an aggregation argument at the end, as we have been using.  However, what if each row is uniquely defined by the `groupby` and `pivot` columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = Row('state', 'industry', 'hq', 'jobs', 'firm')\n",
    "\n",
    "df = sc.parallelize([\n",
    "    row('MI', 'auto', 'domestic', 716, 'A'),\n",
    "    row('MI', 'auto', 'foreign', 123, 'B'),\n",
    "    row('MI', 'auto', 'domestic', 1340, 'C'),\n",
    "    row('MI', 'retail', 'foreign', 12, 'D'),\n",
    "    row('MI', 'retail', 'foreign', 33, 'E'),\n",
    "    row('OH', 'auto', 'domestic', 349, 'F'),\n",
    "    row('OH', 'auto', 'foreign', 101, 'G'),\n",
    "    row('OH', 'auto', 'foreign', 77, 'H'),\n",
    "    row('OH', 'retail', 'domestic', 45, 'I'),\n",
    "    row('OH', 'retail', 'foreign', 12, 'J')\n",
    "    ]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----+----+\n",
      "|state|industry|      hq|jobs|firm|\n",
      "+-----+--------+--------+----+----+\n",
      "|   MI|    auto|domestic| 716|   A|\n",
      "|   MI|    auto| foreign| 123|   B|\n",
      "|   MI|    auto|domestic|1340|   C|\n",
      "|   MI|  retail| foreign|  12|   D|\n",
      "|   MI|  retail| foreign|  33|   E|\n",
      "|   OH|    auto|domestic| 349|   F|\n",
      "|   OH|    auto| foreign| 101|   G|\n",
      "|   OH|    auto| foreign|  77|   H|\n",
      "|   OH|  retail|domestic|  45|   I|\n",
      "|   OH|  retail| foreign|  12|   J|\n",
      "+-----+--------+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now added a unique identifier for each firm, which we will use instead of state and industry as our groupby criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pivot = df.groupBy('firm').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+\n",
      "|firm|domestic|foreign|\n",
      "+----+--------+-------+\n",
      "|   A|     716|   null|\n",
      "|   B|    null|    123|\n",
      "|   C|    1340|   null|\n",
      "|   D|    null|     12|\n",
      "|   E|    null|     33|\n",
      "|   F|     349|   null|\n",
      "|   G|    null|    101|\n",
      "|   H|    null|     77|\n",
      "|   I|      45|   null|\n",
      "|   J|    null|     12|\n",
      "+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It dropped the columns we didn't use anywhere, state and industry, but if we wanted to keep them we can just include them the groupby criteria without changing the logic of the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pivot = df.groupBy('firm', 'state', 'industry').pivot('hq', values=['domestic', 'foreign']).sum('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+--------+-------+\n",
      "|firm|state|industry|domestic|foreign|\n",
      "+----+-----+--------+--------+-------+\n",
      "|   D|   MI|  retail|    null|     12|\n",
      "|   G|   OH|    auto|    null|    101|\n",
      "|   F|   OH|    auto|     349|   null|\n",
      "|   C|   MI|    auto|    1340|   null|\n",
      "|   J|   OH|  retail|    null|     12|\n",
      "|   B|   MI|    auto|    null|    123|\n",
      "|   I|   OH|  retail|      45|   null|\n",
      "|   A|   MI|    auto|     716|   null|\n",
      "|   E|   MI|  retail|    null|     33|\n",
      "|   H|   OH|    auto|    null|     77|\n",
      "+----+-----+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we're doing is still telling it to `sum` each grouping of values, but **each grouping only has a single entry.**  Our data is now reshaped from long to wide, although it may not look \"wide\" just because there are only two entries in our pivot column, `hq`.  If we replaced the `sum` operator with `mean` or `max` or `min`, it wouldn't change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

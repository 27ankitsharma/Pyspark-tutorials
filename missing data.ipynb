{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**_pySpark Basics: Missing Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 21 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: In this guide we'll look at how to handle null and missing values in pySpark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by verifying the Spark Context and loading the SQL Context necessary to work with a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load some real data from CSV to work with.  It helps to know in advance how the dataset handles missing values - are they an empty string, or something else?  Most CSVs will use empty strings, but we can't compute anything on a column that is mixed strings and numbers.  The `null` object in pySpark is what we want, and we can tell it when we import the data to replace the value our data uses to denote missing data with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|',\n",
    "                          nullValue=''\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on the `nullValue=''` line, the empty string can be replaced by whatever your dataset uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how many rows the entire dataframe has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore missing data in pySpark, we need to make sure we're looking in a numerical column - the system does not insert `null` into a column that has a string datatype.  The general point of `null` is so the system knows to skip those rows when doing calculations down a column.  \n",
    "\n",
    "For example, the mean of the series [3, 4, 2, null, 5] is: \n",
    "\n",
    "14 / 4 = 3.5 \n",
    "\n",
    "not: \n",
    "\n",
    "14 / 5 = 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 'bigint'),\n",
       " ('C1', 'string'),\n",
       " ('C2', 'string'),\n",
       " ('C3', 'double'),\n",
       " ('C4', 'double'),\n",
       " ('C5', 'int'),\n",
       " ('C6', 'int'),\n",
       " ('C7', 'int'),\n",
       " ('C8', 'string'),\n",
       " ('C9', 'int'),\n",
       " ('C10', 'string'),\n",
       " ('C11', 'string'),\n",
       " ('C12', 'int'),\n",
       " ('C13', 'string'),\n",
       " ('C14', 'string'),\n",
       " ('C15', 'string'),\n",
       " ('C16', 'string'),\n",
       " ('C17', 'string'),\n",
       " ('C18', 'string'),\n",
       " ('C19', 'string'),\n",
       " ('C20', 'string'),\n",
       " ('C21', 'string'),\n",
       " ('C22', 'string'),\n",
       " ('C23', 'string'),\n",
       " ('C24', 'string'),\n",
       " ('C25', 'string'),\n",
       " ('C26', 'int'),\n",
       " ('C27', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our practice purposes it doesn't matter what this data is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df['C12'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at our earlier command, `df.count()`, we can see that column `C12` is mostly null values - there are 15,860 values in here, out of 3,526,154 rows.  \n",
    "\n",
    "When exploring a dataset we might want to check all our numeric rows for null values.  However, the `isNull()` method can only be called on a column, not an entire dataframe, so I'll write a convenient Python function to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_counts = []       #make an empty list to hold our results\n",
    "for col in df.dtypes:  #iterate through the column data types we saw above\n",
    "    cname = col[0]\n",
    "    ctype = col[1]\n",
    "    if ctype != 'string': #calling isNull() on string columns will just return 0, so we skip them for efficiency\n",
    "        nulls = df.where(df[cname].isNull()).count()\n",
    "        result = tuple([cname, nulls])\n",
    "        null_counts.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 0),\n",
       " ('C3', 0),\n",
       " ('C4', 1945752),\n",
       " ('C5', 0),\n",
       " ('C6', 0),\n",
       " ('C7', 1),\n",
       " ('C9', 0),\n",
       " ('C12', 3510294),\n",
       " ('C26', 3526153)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about Python programming in general, for those who may be new(er) to the language: one of the core precepts of Python is that code should be as easy to read as possible.  For the purpose of clairty I spread that last code block out vertically far more than was strictly necessary.  This bit of code would do the exact same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "null_counts = []\n",
    "for col in df.dtypes:\n",
    "    if col[1] != 'string':\n",
    "        null_counts.append(tuple([col[0], df.where(df[col[0]].isNull()).count()])))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But despite accomplishing the same thing in 4 lines instead of 8, it violates the rules of Python style by looking like an unreadable jumble.  Much more on this can be found in the official Python PEP8 style guide, located at:\n",
    "\n",
    "https://www.python.org/dev/peps/pep-0008/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

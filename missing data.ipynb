{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**_pySpark Basics: Missing Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 21 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: In this guide we'll look at how to handle null and missing values in pySpark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by verifying the Spark Context and loading the SQL Context necessary to work with a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load some real data from CSV to work with.  It helps to know in advance how the dataset handles missing values - are they an empty string, or something else?  Most CSVs will use empty strings, but we can't compute anything on a column that is mixed strings and numbers.  The `null` object in pySpark is what we want, and we can tell it when we import the data to replace the value our data uses to denote missing data with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|',\n",
    "                          nullValue=''\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on the `nullValue=''` line, the empty string can be replaced by whatever your dataset uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how many rows the entire dataframe has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore missing data in pySpark, we need to make sure we're looking in a numerical column - the system does not insert `null` into a column that has a string datatype.  The general point of `null` is so the system knows to skip those rows when doing calculations down a column.  \n",
    "\n",
    "For example, the mean of the series [3, 4, 2, null, 5] is: \n",
    "\n",
    "14 / 4 = 3.5 \n",
    "\n",
    "not: \n",
    "\n",
    "14 / 5 = 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 'bigint'),\n",
       " ('C1', 'string'),\n",
       " ('C2', 'string'),\n",
       " ('C3', 'double'),\n",
       " ('C4', 'double'),\n",
       " ('C5', 'int'),\n",
       " ('C6', 'int'),\n",
       " ('C7', 'int'),\n",
       " ('C8', 'string'),\n",
       " ('C9', 'int'),\n",
       " ('C10', 'string'),\n",
       " ('C11', 'string'),\n",
       " ('C12', 'int'),\n",
       " ('C13', 'string'),\n",
       " ('C14', 'string'),\n",
       " ('C15', 'string'),\n",
       " ('C16', 'string'),\n",
       " ('C17', 'string'),\n",
       " ('C18', 'string'),\n",
       " ('C19', 'string'),\n",
       " ('C20', 'string'),\n",
       " ('C21', 'string'),\n",
       " ('C22', 'string'),\n",
       " ('C23', 'string'),\n",
       " ('C24', 'string'),\n",
       " ('C25', 'string'),\n",
       " ('C26', 'int'),\n",
       " ('C27', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our practice purposes it doesn't matter what this data actually is, so we'll arbitrarily select a numerical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where( df['C12'].isNull() ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command takes the form `df.where(___).count()` where the blank is replaced with the desired condition.  In the code I used some extra spaces in between the brackets just to make this stand out - Python ignores extra horizonal space nestled in commands like that.\n",
    "\n",
    "Note that if we left the `count()` method off the end then it would return an actual dataframe of all rows where column `C12` is `null`.  So if you wanted more than just the count you could explore that subset.\n",
    "\n",
    "When we compare the null count to our earlier command, `df.count()`, we can see that column `C12` is mostly null values - there are 15,860 values in here, out of 3,526,154 rows.  A common need when exploring a dataset might be to check _all_ our numeric rows for null values.  However, the `isNull()` method can only be called on a column, not an entire dataframe, so I'll write a convenient Python function to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nulls(df):\n",
    "    null_counts = []          #make an empty list to hold our results\n",
    "    for col in df.dtypes:     #iterate through the column data types we saw above\n",
    "        cname = col[0]\n",
    "        ctype = col[1]\n",
    "        if ctype != 'string': #calling isNull() on string columns will do the work but return 0, so we skip them for efficiency\n",
    "            nulls = df.where( df[cname].isNull() ).count()\n",
    "            result = tuple([cname, nulls])\n",
    "            null_counts.append(result)\n",
    "    return null_counts\n",
    "\n",
    "null_counts = count_nulls(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 0),\n",
       " ('C3', 0),\n",
       " ('C4', 1945752),\n",
       " ('C5', 0),\n",
       " ('C6', 0),\n",
       " ('C7', 1),\n",
       " ('C9', 0),\n",
       " ('C12', 3510294),\n",
       " ('C26', 3526153)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about Python programming in general, for those who may be new(er) to the language: one of the core precepts of Python is that code should be as easy to read as possible.  For the purpose of clairty I spread the code in that last function out vertically far more than was strictly necessary.  This bit of code would do the exact same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "null_counts = []\n",
    "for col in df.dtypes:\n",
    "    if col[1] != 'string':\n",
    "        null_counts.append(tuple([col[0], df.where(df[col[0]].isNull()).count()])))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But despite accomplishing the same thing in 4 lines instead of 8, it violates the rules of Python style by looking like an unreadable jumble.  Much more on this can be found in the official Python PEP8 style guide, located at:\n",
    "\n",
    "https://www.python.org/dev/peps/pep-0008/\n",
    "\n",
    "If you'll be writing much Python code it's definitely worth looking over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are three things we can do with our `null` values now that we know what's in our dataframe.  We can ignore them, we can drop them, or we can replace them.  Remember, pySpark dataframes are immutable, so we can't actually change the original dataset.  All operations return an entirely new dataframe, though we can tell it to overwrite the existing one with `df = df.some_operation()` which ends up roughly equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_drops = df.dropna(how='all', thresh=None, subset=['C4', 'C12', 'C26'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580403"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drops.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.dropna()` method can have three arguments, all of which are optional.  The `how` argument can be `any` or `all`; the first drops a row if _any_ value is `null`, the second drops a row only if _all_ values are.  \n",
    "\n",
    "The `thresh` argument sets a threshold for the number of `null` entries in a row before it drops it.  It is set to an integer that specifies how many non-null arguments the row must have; if it has less than that figure it drops.\n",
    "\n",
    "And finally the `subset` argument takes a list of columns that you want to look in for `null` values.  It does not actually subset the dataframe; it just checks in those three columns, then drops the row for the entire dataframe if that subset meets the criteria.\n",
    "\n",
    "So we can see above that once we drop all rows that have `null` for columns `C4`, `C12` and `C26`, we're left with 1,580,403 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fill = df.fillna(0, subset=['C12'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line goes through all of column `C12` and replaces `null` values with a zero.  To verify we re-run the command on our new dataframe to count nulls that we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fill.where( df_fill['C12'].isNull() ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it replaced all 3,510,294 nulls we found earlier.  The first term in `fillna()` can be most any type and any value, and the subset list can be left off if the fill should be applied to all columns.  Note that `df.replace(a, b)` does this same thing, only you specify `a` as the value to be replaced and `b` as the replacement.  It also accepts the optional subset list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Dataframe Concepts_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 8 Aug 2016, Spark v2.0_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will explore some basic concepts necessary for working with many dataframe operations, in particular `groupBy` and `persist`._\n",
    "\n",
    "_Main operations used: read.load, withColumn, groupBy, persist, cache, unpersist_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark does its RDD computations in what is called a _lazy_ fashion.  That is, when you tell it to do things to an RDD it _doesn't do them right away._  Instead it makes sure they're valid commands, then stacks them up until you actually ask it to return a value or a dataframe to you.  This is called a _lineage_ in Spark, and means an RDD isn't a store of data, it's a store of instructions.  \n",
    "\n",
    "Let's see it in action.  First we'll load up the same dataframe we did in basics 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('s3://ui-spark-data/Performance_2015Q1.txt', header=False, inferSchema=True, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes a while, because `read.csv()` returns a dataframe.  But now let's try some numerical operations on a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'bigint'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'double'),\n",
       " ('_c4', 'double'),\n",
       " ('_c5', 'int'),\n",
       " ('_c6', 'int'),\n",
       " ('_c7', 'int'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'int'),\n",
       " ('_c10', 'string'),\n",
       " ('_c11', 'string'),\n",
       " ('_c12', 'int'),\n",
       " ('_c13', 'string'),\n",
       " ('_c14', 'string'),\n",
       " ('_c15', 'string'),\n",
       " ('_c16', 'string'),\n",
       " ('_c17', 'string'),\n",
       " ('_c18', 'string'),\n",
       " ('_c19', 'string'),\n",
       " ('_c20', 'string'),\n",
       " ('_c21', 'string'),\n",
       " ('_c22', 'string'),\n",
       " ('_c23', 'string'),\n",
       " ('_c24', 'string'),\n",
       " ('_c25', 'string'),\n",
       " ('_c26', 'int'),\n",
       " ('_c27', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_new = df.withColumn('New_c12', df['_c12'] ** 2)    #New_c12 = _c12^2\n",
    "df_new = df_new.withColumn('New_c12', df_new['New_c12'] + df_new['_c12']) #New_c12 = New_c12 + _c12\n",
    "df_grp = df_new.groupBy('_c2')\n",
    "df_avg = df_grp.avg('_c3', '_c5', '_c6', '_c12', 'New_c12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we performed two (arbitrary) math operations then perform a `groupBy` operation over the entries in `_c2` (more on groupBy in a minute) while asking it to calculate averages for six numeric columns within those groups.  \n",
    "\n",
    "However, notice that the the code block finished nearly instantly - we added a simple timer to print out how long it took by using the Jupyter `time` \"magic\" - despite there being over 3.5 million rows of data.  This is _lazy_ computing - **nothing was actually computed here because we are just stacking instructions up.**  All pySpark did was make sure they were valid instructions.  Now let's see what happens if we tell it to `show` us the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|                 _c2|          avg(_c3)|            avg(_c5)|          avg(_c6)|         avg(_c12)|      avg(New_c12)|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|  QUICKEN LOANS INC.|  4.35347951590827|-0.08899247348614438| 358.5689787889155|              null|              null|\n",
      "|NATIONSTAR MORTGA...| 4.172708234075604| 0.39047125841532887| 359.5821853961678|               1.0|               2.0|\n",
      "|WELLS FARGO BANK,...| 4.266629427172305|  0.6704475572258285|359.25937820293814|              null|              null|\n",
      "|FANNIE MAE/SETERU...| 4.433333333333333|   9.333333333333334| 350.6666666666667|              null|              null|\n",
      "|DITECH FINANCIAL LLC| 4.192566550005296|   5.147629653197582| 354.7811008590519|               1.0|               2.0|\n",
      "|SENECA MORTGAGE S...|4.1412100378136785| -0.2048814025438295|360.20075627363354|              null|              null|\n",
      "|SUNTRUST MORTGAGE...| 4.102661585365845|  0.8241234756097561| 359.1453887195122|              null|              null|\n",
      "|ROUNDPOINT MORTGA...|4.2378581711209815|   5.153408024034549| 354.8269387244163|               1.0|               2.0|\n",
      "|      PENNYMAC CORP.| 4.215393569844791| 0.14966740576496673| 359.8470066518847|              null|              null|\n",
      "|PHH MORTGAGE CORP...|  4.15648032936871|  0.9780420860018298|359.02195791399816|              null|              null|\n",
      "|MATRIX FINANCIAL ...| 4.100071062740077|   6.566794707639778| 353.4229620145113|               1.0|               2.0|\n",
      "|               OTHER| 4.128346179641528| 0.11480465916297489| 359.8345750772193|               1.0|               2.0|\n",
      "|  CITIMORTGAGE, INC.| 4.101532687651331|   0.338498789346247|359.41670702179175|              null|              null|\n",
      "|PINGORA LOAN SERV...| 4.085800941535713|   7.573573382530696|352.40886824861633|1.0471698113207548|2.3773584905660377|\n",
      "|JP MORGAN CHASE B...| 4.327598085711821|  1.6553418987669224| 358.3384495990342|              null|              null|\n",
      "|      PNC BANK, N.A.|  4.37174256799494|  1.1707779886148009|358.78747628083494|               1.0|               2.0|\n",
      "|FREEDOM MORTGAGE ...| 4.182932197672456|    8.56265812109968|351.29583403609377|1.0909090909090908| 2.727272727272727|\n",
      "|                    | 4.178960023726847|  5.6264681794400015|354.21486809483747|1.0892949047864127| 2.723623262995368|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes a bit longer to run, because when you executed `show` you asked for a dataframe to be returned to you, which meant **Spark went back and caclulated the three previous operations.**  You could have done any number of intermediate steps similar to those before calling `show` and they all would have been lazy operations that finished nearly instantly, until `show` ran them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this would just be a background peculiarity, except that we have some control over the process.  If you imagine your _lineage_ as a straight line of instructions leading from your source data to your ouput, **we can use the `persist()` method to create a point for branching.**  Essentially it tells Spark \"follow the instructions to this point, then _hold these results_ because I'm going to come back to them again.\"\n",
    "\n",
    "Let's redo the previous code block with a `persist()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_new = df.withColumn('New_c12', df['_c12'] ** 2)    #New_c12 = _c12^2\n",
    "df_new = df_new.withColumn('New_c12', df_new['New_c12'] + df_new['_c12']) #New_c12 = New_c12 + _c12\n",
    "df_new.persist()\n",
    "df_grp = df_new.groupBy('_c2')\n",
    "df_avg = df_grp.avg('_c3', '_c5', '_c6', '_c12', 'New_c12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `persist` command adds very little overhead in this case, finishing in in well under a second.  Now we call `show` again to force it to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|                 _c2|          avg(_c3)|            avg(_c5)|          avg(_c6)|         avg(_c12)|      avg(New_c12)|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|  QUICKEN LOANS INC.|  4.35347951590827|-0.08899247348614438| 358.5689787889155|              null|              null|\n",
      "|NATIONSTAR MORTGA...| 4.172708234075604| 0.39047125841532887| 359.5821853961678|               1.0|               2.0|\n",
      "|WELLS FARGO BANK,...| 4.266629427172305|  0.6704475572258285|359.25937820293814|              null|              null|\n",
      "|FANNIE MAE/SETERU...| 4.433333333333334|   9.333333333333334| 350.6666666666667|              null|              null|\n",
      "|DITECH FINANCIAL LLC| 4.192566550005296|   5.147629653197582| 354.7811008590519|               1.0|               2.0|\n",
      "|SENECA MORTGAGE S...| 4.141210037813679| -0.2048814025438295|360.20075627363354|              null|              null|\n",
      "|SUNTRUST MORTGAGE...| 4.102661585365845|  0.8241234756097561| 359.1453887195122|              null|              null|\n",
      "|ROUNDPOINT MORTGA...|4.2378581711209815|   5.153408024034549| 354.8269387244163|               1.0|               2.0|\n",
      "|      PENNYMAC CORP.| 4.215393569844789| 0.14966740576496673| 359.8470066518847|              null|              null|\n",
      "|PHH MORTGAGE CORP...|  4.15648032936871|  0.9780420860018298|359.02195791399816|              null|              null|\n",
      "|MATRIX FINANCIAL ...| 4.100071062740077|   6.566794707639778| 353.4229620145113|               1.0|               2.0|\n",
      "|               OTHER| 4.128346179641527| 0.11480465916297489| 359.8345750772193|               1.0|               2.0|\n",
      "|  CITIMORTGAGE, INC.| 4.101532687651332|   0.338498789346247|359.41670702179175|              null|              null|\n",
      "|PINGORA LOAN SERV...| 4.085800941535713|   7.573573382530696|352.40886824861633|1.0471698113207548|2.3773584905660377|\n",
      "|JP MORGAN CHASE B...| 4.327598085711821|  1.6553418987669224| 358.3384495990342|              null|              null|\n",
      "|      PNC BANK, N.A.|  4.37174256799494|  1.1707779886148009|358.78747628083494|               1.0|               2.0|\n",
      "|FREEDOM MORTGAGE ...| 4.182932197672456|    8.56265812109968|351.29583403609377|1.0909090909090908| 2.727272727272727|\n",
      "|                    | 4.178960023726847|  5.6264681794400015|354.21486809483747|1.0892949047864127| 2.723623262995368|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the groupBy averages this way took a bit longer because of the `persist` overhead.  But now let's back up and, in addition to the mean, lets also get the sums of our groupBy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 38.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sum = df_grp.sum('_c3', '_c5', '_c6', '_c12', 'New_c12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the *lazy* portion, now we make it execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+---------+------------+\n",
      "|                 _c2|            sum(_c3)|sum(_c5)|  sum(_c6)|sum(_c12)|sum(New_c12)|\n",
      "+--------------------+--------------------+--------+----------+---------+------------+\n",
      "|  QUICKEN LOANS INC.|    101801.764999999|   -2081|   8384777|     null|        null|\n",
      "|NATIONSTAR MORTGA...|  40287.497999999956|    3770|   3471766|        2|         4.0|\n",
      "|WELLS FARGO BANK,...|  187326.36500000005|   29436|  15773283|     null|        null|\n",
      "|FANNIE MAE/SETERU...|  26.599999999999998|      56|      2104|     null|        null|\n",
      "|DITECH FINANCIAL LLC|  39531.709999999934|   48537|   3345231|       41|        82.0|\n",
      "|SENECA MORTGAGE S...|  24093.559999999983|   -1192|   2095648|     null|        null|\n",
      "|SUNTRUST MORTGAGE...|  21530.767999999953|    4325|   1884795|     null|        null|\n",
      "|ROUNDPOINT MORTGA...|   67708.25999999992|   82336|   5669070|       74|       148.0|\n",
      "|      PENNYMAC CORP.|  15209.140000000003|     540|   1298328|     null|        null|\n",
      "|PHH MORTGAGE CORP...|            9086.066|    2138|    784822|     null|        null|\n",
      "|MATRIX FINANCIAL ...|           19212.933|   30772|   1656140|       16|        32.0|\n",
      "|               OTHER|   904855.0440000097|   25163|  78868902|       21|        42.0|\n",
      "|  CITIMORTGAGE, INC.|  16939.329999999998|    1398|   1484391|     null|        null|\n",
      "|PINGORA LOAN SERV...|   64224.70499999988|  119049|   5539515|      111|       252.0|\n",
      "|JP MORGAN CHASE B...|  50187.154999999984|   19197|   4155651|     null|        null|\n",
      "|      PNC BANK, N.A.|            6911.725|    1851|    567243|        1|         2.0|\n",
      "|FREEDOM MORTGAGE ...|   24800.60499999999|   50768|   2082833|       60|       150.0|\n",
      "|                    |1.3139130894999936E7|17690263|1113692280|    16932|     42336.0|\n",
      "+--------------------+--------------------+--------+----------+---------+------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was dramatically faster than the calculation showing the averages - 1.49 seconds versus over 18 seconds.  This is because Spark kept the intermediate results up to our `persist()` call from when we calculated the averages, and thus only had to run the code that came after that.  We can now do as many different branches of operations as we want stemming from `df_new` and since we persisted it, all the code before can be skipped.\n",
    "\n",
    "There is no need for persisting if there is no branching.  In fact, as we saw, `persist` adds a bit of overhead to the process, and so is actually a hinderance if you're not going to be utilizing the branch point.  As a matter of good practice, and to free up more resources, you can call `.unpersist()` on a persisted object to drop it from storage when done with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The trailing ; simply gags the output from the command. We don't need to see the summary of what we just unpersisted)\n",
    "\n",
    "Also note that `cache()` is essentially a synonym for `persist()`, except it specifies storing the checkpoint in memory for the fastest recall, while persisting allows Spark to swap some of the checkpoint to disk if necessary.  Obviously `cache()` only works if the dataframe you are forcing it to hold is small enough that it can fit in the memory of each node, so use it with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a bit more on `groupBy`.  Hopefully the usage above has given you some insight into how it works.  In short, `groupBy` is the vehicle for aggregation in a dataframe.  A `groupBy` object is, in itself, incomplete.  So, the line in the code block where we introduced a `persist()` above that looks like this:\n",
    "\n",
    "`df_grp = df_new.groupBy('_c2')`\n",
    "\n",
    "which generates a `groupBy` object where the data is grouped around the unique values found in column `C2`, but it is just a foundation.  It is like the sentence _\"We are going to group our data up by the unique values found in column C2, and then...\"_  The sentence is unfinished!  The next line of code contains the rest:\n",
    "\n",
    "`df_avg = df_grp.avg('_c3', '_c5', '_c6', '_c12', 'New_c12')`\n",
    "\n",
    "Or to finish the sentence, _\"... calculate the averages for these five columns within each group.\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

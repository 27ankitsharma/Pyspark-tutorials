{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Dataframe Concepts_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 22 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will explore some basic concepts necessary for working with many dataframe operations, in particular `groupBy` and `persist`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes are just a convenient wrapper over the basic structure of data that Spark uses, which is called an RDD (Resilient Distributed Dataset).  It's _resilient_ because a core benefit of Spark is how it handles network and hardware failures - when you're working with a small dataset on your desktop computer, this rarely becomes an issue.  However, one of the core benefits to Spark is that it's _distributed_ across many computers, and as that number rises the number of failures you will encounter will naturally rise as well.  By some estimates if you have 10,000 computers working together - which Spark can handle - you should expect one system to fail _every day_.  And that's not even counting things like a system freezing or other software problems, or a problem with traffic over your network.\n",
    "\n",
    "Spark's central manager (the \"master\") is very smart with this regard.  First, all data is in multiple locations (usually three), so no one failure can threaten your data.  But beyond that, the master keeps track of all of the tasks it has sent to the nodes to do, and if the node doing the work breaks the master just sends the task to a different node.  This is completely transparent to the user.  \n",
    "\n",
    "Even more than that, if the node doing the work doesn't reply in a reasonable amount of time, the master will send the job out to someone else to finish and then take the result from whoever reports back first.\n",
    "\n",
    "Spark does its RDD computations in what is called a _lazy_ fashion.  That is, when you tell it to do things to an RDD it _doesn't do them right away._  Instead it makes sure they're valid commands, then stacks them up until you actually ask it to return a value or a dataframe to you.  This is called a _lineage_ in Spark, and means an RDD isn't a store of data, it's a store of instructions.  \n",
    "\n",
    "Let's see it in action.  First we'll load up the same dataframe we did in basics 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes a while, because `read.load.()` returns a dataframe.  But now let's try some numerical operations on a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 'bigint'),\n",
       " ('C1', 'string'),\n",
       " ('C2', 'string'),\n",
       " ('C3', 'double'),\n",
       " ('C4', 'double'),\n",
       " ('C5', 'int'),\n",
       " ('C6', 'int'),\n",
       " ('C7', 'int'),\n",
       " ('C8', 'string'),\n",
       " ('C9', 'int'),\n",
       " ('C10', 'string'),\n",
       " ('C11', 'string'),\n",
       " ('C12', 'int'),\n",
       " ('C13', 'string'),\n",
       " ('C14', 'string'),\n",
       " ('C15', 'string'),\n",
       " ('C16', 'string'),\n",
       " ('C17', 'string'),\n",
       " ('C18', 'string'),\n",
       " ('C19', 'string'),\n",
       " ('C20', 'string'),\n",
       " ('C21', 'string'),\n",
       " ('C22', 'string'),\n",
       " ('C23', 'string'),\n",
       " ('C24', 'string'),\n",
       " ('C25', 'string'),\n",
       " ('C26', 'int'),\n",
       " ('C27', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = df.withColumn('New_C12', df['C12'] ** 2)    #New_C12 = C12^2\n",
    "df_new = df_new.withColumn('New_C12', df_new['New_C12'] + df_new['C12']) #New_C12 = New_C12 + C12\n",
    "df_grp = df_new.groupBy('C2')\n",
    "df_avg = df_grp.avg('C3', 'C5', 'C6', 'C12', 'New_C12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we performed two (arbitrary) math operations then perform a `groupBy` operation over the entries in `C2` (more on groupBy in a minute) while asking it to calculate averages for six numeric columns within those groups.  \n",
    "\n",
    "However, you hopefully noticed when running that code block that it finished instantly - despite there being over 3.5 million rows of data.  This is _lazy_ computing - you're just stacking instructions up.  Now let's see what happens if we tell it to `show` us the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|                  C2|           avg(C3)|             avg(C5)|           avg(C6)|          avg(C12)|      avg(New_C12)|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|      PNC BANK, N.A.| 4.371742567994939|  1.1707779886148009|358.78747628083494|               1.0|               2.0|\n",
      "|PHH MORTGAGE CORP...| 4.156480329368712|  0.9780420860018298|359.02195791399816|              null|              null|\n",
      "|  QUICKEN LOANS INC.| 4.353479515908324|-0.08899247348614438| 358.5689787889155|              null|              null|\n",
      "|  CITIMORTGAGE, INC.| 4.101532687651331|   0.338498789346247|359.41670702179175|              null|              null|\n",
      "|WELLS FARGO BANK,...| 4.266629427172304|  0.6704475572258285|359.25937820293814|              null|              null|\n",
      "|JP MORGAN CHASE B...| 4.327598085711821|  1.6553418987669224| 358.3384495990342|              null|              null|\n",
      "|ROUNDPOINT MORTGA...| 4.237858171120981|   5.153408024034549| 354.8269387244163|               1.0|               2.0|\n",
      "|SUNTRUST MORTGAGE...| 4.102661585365834|  0.8241234756097561| 359.1453887195122|              null|              null|\n",
      "|               OTHER| 4.128346179641434| 0.11480465916297489| 359.8345750772193|               1.0|               2.0|\n",
      "|PINGORA LOAN SERV...| 4.085800941535712|   7.573573382530696|352.40886824861633|1.0471698113207548|2.3773584905660377|\n",
      "|FANNIE MAE/SETERU...| 4.433333333333334|   9.333333333333334| 350.6666666666667|              null|              null|\n",
      "|SENECA MORTGAGE S...| 4.141210037813677| -0.2048814025438295|360.20075627363354|              null|              null|\n",
      "|                    | 4.178960023728788|  5.6264681794400015|354.21486809483747|1.0892949047864127| 2.723623262995368|\n",
      "|      PENNYMAC CORP.| 4.215393569844788| 0.14966740576496673| 359.8470066518847|              null|              null|\n",
      "|NATIONSTAR MORTGA...| 4.172708234075602| 0.39047125841532887| 359.5821853961678|               1.0|               2.0|\n",
      "|MATRIX FINANCIAL ...| 4.100071062740076|   6.566794707639778| 353.4229620145113|               1.0|               2.0|\n",
      "|DITECH FINANCIAL LLC| 4.192566550005294|   5.147629653197582| 354.7811008590519|               1.0|               2.0|\n",
      "|FREEDOM MORTGAGE ...|4.1829321976724545|    8.56265812109968|351.29583403609377|1.0909090909090908| 2.727272727272727|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have taken a long time to run, because when you executed `show` you asked for a dataframe to be returned to you, which meant Spark went back and caclulated the three previous operations.  You could have done any number of intermediate steps similar to those before calling `show` and they all would have been lazy operations that finished instantly, until `show` ran them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this would just be a background peculiarity that you only notice by which commands execute fast and which ones slow, except that we have some control over the process.  If you imagine your _lineage_ as a straight line leading from your source data to your ouput, we can use the `persist()` method to create a point for branching.  Essentially it tells Spark \"follow the instructions to this point, then _hold these results_ because I'm going to come back to them again.\"\n",
    "\n",
    "Let's redo the previous code block with a `persist()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = df.withColumn('New_C12', df['C12'] ** 2)    #New_C12 = C12^2\n",
    "df_new = df_new.withColumn('New_C12', df_new['New_C12'] + df_new['C12']) #New_C12 = New_C12 + C12\n",
    "df_new.persist()\n",
    "df_grp = df_new.groupBy('C2')\n",
    "df_avg = df_grp.avg('C3', 'C5', 'C6', 'C12', 'New_C12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|                  C2|           avg(C3)|             avg(C5)|           avg(C6)|          avg(C12)|      avg(New_C12)|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|      PNC BANK, N.A.| 4.371742567994939|  1.1707779886148009|358.78747628083494|               1.0|               2.0|\n",
      "|PHH MORTGAGE CORP...| 4.156480329368712|  0.9780420860018298|359.02195791399816|              null|              null|\n",
      "|  QUICKEN LOANS INC.| 4.353479515908324|-0.08899247348614438| 358.5689787889155|              null|              null|\n",
      "|  CITIMORTGAGE, INC.| 4.101532687651331|   0.338498789346247|359.41670702179175|              null|              null|\n",
      "|WELLS FARGO BANK,...| 4.266629427172304|  0.6704475572258285|359.25937820293814|              null|              null|\n",
      "|JP MORGAN CHASE B...| 4.327598085711822|  1.6553418987669224| 358.3384495990342|              null|              null|\n",
      "|ROUNDPOINT MORTGA...| 4.237858171120981|   5.153408024034549| 354.8269387244163|               1.0|               2.0|\n",
      "|SUNTRUST MORTGAGE...| 4.102661585365834|  0.8241234756097561| 359.1453887195122|              null|              null|\n",
      "|               OTHER| 4.128346179641434| 0.11480465916297489| 359.8345750772193|               1.0|               2.0|\n",
      "|PINGORA LOAN SERV...| 4.085800941535712|   7.573573382530696|352.40886824861633|1.0471698113207548|2.3773584905660377|\n",
      "|FANNIE MAE/SETERU...| 4.433333333333334|   9.333333333333334| 350.6666666666667|              null|              null|\n",
      "|SENECA MORTGAGE S...| 4.141210037813677| -0.2048814025438295|360.20075627363354|              null|              null|\n",
      "|                    | 4.178960023728788|  5.6264681794400015|354.21486809483747|1.0892949047864127| 2.723623262995368|\n",
      "|      PENNYMAC CORP.| 4.215393569844787| 0.14966740576496673| 359.8470066518847|              null|              null|\n",
      "|NATIONSTAR MORTGA...| 4.172708234075603| 0.39047125841532887| 359.5821853961678|               1.0|               2.0|\n",
      "|MATRIX FINANCIAL ...| 4.100071062740075|   6.566794707639778| 353.4229620145113|               1.0|               2.0|\n",
      "|DITECH FINANCIAL LLC| 4.192566550005294|   5.147629653197582| 354.7811008590519|               1.0|               2.0|\n",
      "|FREEDOM MORTGAGE ...|4.1829321976724545|    8.56265812109968|351.29583403609377|1.0909090909090908| 2.727272727272727|\n",
      "+--------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the groupBy averages this way is no different than the first way we did it - Spark executes the entire lineage.  But now let's get the sums of our groupBy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum = df_grp.sum('C3', 'C5', 'C6', 'C12', 'New_C12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+--------+------------+\n",
      "|                  C2|             sum(C3)| sum(C5)|   sum(C6)|sum(C12)|sum(New_C12)|\n",
      "+--------------------+--------------------+--------+----------+--------+------------+\n",
      "|      PNC BANK, N.A.|   6911.724999999999|    1851|    567243|       1|         2.0|\n",
      "|PHH MORTGAGE CORP...|   9086.066000000004|    2138|    784822|    null|        null|\n",
      "|  QUICKEN LOANS INC.|  101801.76500000026|   -2081|   8384777|    null|        null|\n",
      "|  CITIMORTGAGE, INC.|  16939.329999999998|    1398|   1484391|    null|        null|\n",
      "|WELLS FARGO BANK,...|          187326.365|   29436|  15773283|    null|        null|\n",
      "|JP MORGAN CHASE B...|   50187.15499999999|   19197|   4155651|    null|        null|\n",
      "|ROUNDPOINT MORTGA...|   67708.25999999991|   82336|   5669070|      74|       148.0|\n",
      "|SUNTRUST MORTGAGE...|  21530.767999999898|    4325|   1884795|    null|        null|\n",
      "|               OTHER|    904855.043999989|   25163|  78868902|      21|        42.0|\n",
      "|PINGORA LOAN SERV...|  64224.704999999856|  119049|   5539515|     111|       252.0|\n",
      "|FANNIE MAE/SETERU...|                26.6|      56|      2104|    null|        null|\n",
      "|SENECA MORTGAGE S...|  24093.559999999972|   -1192|   2095648|    null|        null|\n",
      "|                    |1.3139130895006036E7|17690263|1113692280|   16932|     42336.0|\n",
      "|      PENNYMAC CORP.|  15209.139999999994|     540|   1298328|    null|        null|\n",
      "|NATIONSTAR MORTGA...|   40287.49799999994|    3770|   3471766|       2|         4.0|\n",
      "|MATRIX FINANCIAL ...|  19212.932999999994|   30772|   1656140|      16|        32.0|\n",
      "|DITECH FINANCIAL LLC|   39531.70999999992|   48537|   3345231|      41|        82.0|\n",
      "|FREEDOM MORTGAGE ...|   24800.60499999998|   50768|   2082833|      60|       150.0|\n",
      "+--------------------+--------------------+--------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you noticed that was significantly faster than calculating showing the averages.  This is because Spark kept the intermediate results up to our `persist()` call from when we calculated the averages, and thus only had to run the code that came after that.  We can now do as many different branches of operations as we want stemming from `df_new` and since we persisted it, all the code before can be skipped.\n",
    "\n",
    "There is no need for persisting if there is no branching.  As a matter of good practice, and to free up more resources, you can call `.unpersist()` on a persisted object to drop it from storage when done with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new.unpersist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The trailing ; simply gags the output from the command. We don't need to see the summary of what we just unpersisted)\n",
    "\n",
    "Also note that `cache()` is essentially a synonym for `persist()`, except it specifies storing the checkpoint in memory for the fastest recall, while persisting allows Spark to swap some of the checkpoint to disk if necessary.  Obviously `cache()` only works if the dataframe you are forcing it to hold is small enough that it can fit in the memory of each node, so use it with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a bit more on `groupBy`.  Hopefully the usage above has given you some insight into how it works.  In short, `groupBy` is the vehicle for aggregation in a dataframe.  A `groupBy` object is, in itself, incomplete.  So, the line in the code block where we introduced a `persist()` above that looks like this:\n",
    "\n",
    "`df_grp = df_new.groupBy('C2')`\n",
    "\n",
    "generates a `groupBy` object where the data is grouped around the unique values found in column `C2`, but it is just a foundation.  It is like the sentence _\"We are going to group our data up by the unique values found in column C2, and then...\"_  The sentence is unfinished!  The next line of code contains the rest:\n",
    "\n",
    "`df_avg = df_grp.avg('C3', 'C5', 'C6', 'C12', 'New_C12')`\n",
    "\n",
    "Or to finish the sentence, _\"... calculate the averages for these five columns within each group.\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

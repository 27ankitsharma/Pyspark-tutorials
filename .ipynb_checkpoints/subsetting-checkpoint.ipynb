{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Subsetting Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 28 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will go over filtering your data based on a specified criteria in order to get a subset_\n",
    "\n",
    "_Main operations used: dtypes, take, show, select, drop, filter/where, sample_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some basic setup, importing the SQL structure that supports the dataframes we'll be using.  Note that `sc`, the Spark Context, is created automatically when the cluster is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load in some real data from an S3 bucket (the same data used in the csv tutorial), allowing pySpark to auto determine the schema, then take a quick peak at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 'bigint'),\n",
       " ('C1', 'string'),\n",
       " ('C2', 'string'),\n",
       " ('C3', 'double'),\n",
       " ('C4', 'double'),\n",
       " ('C5', 'int'),\n",
       " ('C6', 'int'),\n",
       " ('C7', 'int'),\n",
       " ('C8', 'string'),\n",
       " ('C9', 'int'),\n",
       " ('C10', 'string'),\n",
       " ('C11', 'string'),\n",
       " ('C12', 'int'),\n",
       " ('C13', 'string'),\n",
       " ('C14', 'string'),\n",
       " ('C15', 'string'),\n",
       " ('C16', 'string'),\n",
       " ('C17', 'string'),\n",
       " ('C18', 'string'),\n",
       " ('C19', 'string'),\n",
       " ('C20', 'string'),\n",
       " ('C21', 'string'),\n",
       " ('C22', 'string'),\n",
       " ('C23', 'string'),\n",
       " ('C24', 'string'),\n",
       " ('C25', 'string'),\n",
       " ('C26', 'int'),\n",
       " ('C27', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(C0=100002091588, C1=u'01/01/2015', C2=u'OTHER', C3=4.125, C4=None, C5=0, C6=360, C7=360, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'02/01/2015', C2=u'', C3=4.125, C4=None, C5=1, C6=359, C7=359, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'03/01/2015', C2=u'', C3=4.125, C4=None, C5=2, C6=358, C7=358, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'04/01/2015', C2=u'', C3=4.125, C4=None, C5=3, C6=357, C7=357, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'05/01/2015', C2=u'', C3=4.125, C4=None, C5=4, C6=356, C7=356, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u'')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: this output looks messy because `take` doesn't format the results, it just shows you the row.  It can be formatted\n",
    "#nicely with `show()`, but due to the width of this it will still look messy.  See below for `show()` in action.\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest subsettings is done by selecting just a few of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_select = df.select(col('C0'), col('C1'), col('C3'), col('C9'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+-----+\n",
      "|          C0|        C1|   C3|   C9|\n",
      "+------------+----------+-----+-----+\n",
      "|100002091588|01/01/2015|4.125|16740|\n",
      "|100002091588|02/01/2015|4.125|16740|\n",
      "|100002091588|03/01/2015|4.125|16740|\n",
      "|100002091588|04/01/2015|4.125|16740|\n",
      "|100002091588|05/01/2015|4.125|16740|\n",
      "+------------+----------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note: `show()` defaults to showing you the first 20 rows, but here we've specified only 5\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do the same thing by dropping, which is convenient if we want to keep more columns than we want to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_drop = df_select.drop(col('C3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|          C0|        C1|   C9|\n",
      "+------------+----------+-----+\n",
      "|100002091588|01/01/2015|16740|\n",
      "|100002091588|02/01/2015|16740|\n",
      "|100002091588|03/01/2015|16740|\n",
      "|100002091588|04/01/2015|16740|\n",
      "|100002091588|05/01/2015|16740|\n",
      "+------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drop.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to subset by rows also, for example by specifying a conditional:\n",
    "<a id=\"filter_rows\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|               C6|\n",
      "+-------+-----------------+\n",
      "|  count|          3526154|\n",
      "|   mean|354.7084951479714|\n",
      "| stddev|4.011812510792076|\n",
      "|    min|              292|\n",
      "|    max|              480|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that we have to use .show() at the end of .describe(), because .describe() returns a new dataframe with the information.\n",
    "#In many other programs, such as Stata, `describe` returns a formatted table.  Here, `summary` and `C6` are actually column names\n",
    "df.describe('C6').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note that `where` is an alias for `filter`: you can use them interchangeably\n",
    "df_filter = df.filter(df['C6'] < 358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                C6|\n",
      "+-------+------------------+\n",
      "|  count|           2598037|\n",
      "|   mean|353.15604897081914|\n",
      "| stddev| 3.517021305688398|\n",
      "|    min|               292|\n",
      "|    max|               357|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.describe('C6').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same proceedure for multiple conditions and columns using standard logical operators, and this time using `where` as the alias for `filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_filter = df.where((df['C6'] > 340) & (df['C5'] < 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|                C6|                C5|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           1254131|           1254131|\n",
      "|   mean|358.48713810598736| 1.474693632483369|\n",
      "| stddev|1.3789619103497548|1.2067831502138442|\n",
      "|    min|               341|                -1|\n",
      "|    max|               361|                 3|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.describe('C6', 'C5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, you might want to take a random sample of rows.  This can be particularlly useful, for example, if your data is large enough to require more expensive clusters to be spun up to work with it all.  Take a more digestable sampling of the whole, do your intermediate work and testing using it on a cheaper cluster, then when it's all ready spin up the more expensive cluster for a final run on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can pass three arguments into sample: the first is a boolean, which is True to sample with replacement, False without.\n",
    "#the second is the fraction of the dataset to take, in this case 5%, and the third is an optional random seed.  if\n",
    "#you specify any integer here then someone else performing the same random operation that specifies the same seed\n",
    "#will get the same result.  if no seed is passed then the exact random sampling can't be duplicated.\n",
    "\n",
    "df_sample = df.sample(False, 0.05, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                C6|\n",
      "+-------+------------------+\n",
      "|  count|            176428|\n",
      "|   mean| 354.7217051715147|\n",
      "| stddev|3.9450655094773754|\n",
      "|    min|               299|\n",
      "|    max|               361|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.describe('C6').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this to our original summary stats on unfiltered column C6 from above, you'll see it does a pretty good job maintaining the mean and stddev in a sample of only 5% of the data.  You can then write this to a new file in an S3 bucket and work with it instead of the whole data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

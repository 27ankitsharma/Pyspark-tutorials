{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Summary Statistics_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 3 Aug 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: Here we will cover several common ways to summarize data.  Many of these methods have been dicussed in other tutorials in different contexts._\n",
    "\n",
    "_Main operations used: `describe`, `skewness`, `kurtosis`, `collect`, `select`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the same csv data we've been using in many other tutorials, then pare it down to a manageable subset for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['C0', 'C2', 'C3', 'C4', 'C5', 'C6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+---+\n",
      "|          C0|   C2|   C3|  C4| C5| C6|\n",
      "+------------+-----+-----+----+---+---+\n",
      "|100002091588|OTHER|4.125|null|  0|360|\n",
      "|100002091588|     |4.125|null|  1|359|\n",
      "|100002091588|     |4.125|null|  2|358|\n",
      "|100002091588|     |4.125|null|  3|357|\n",
      "|100002091588|     |4.125|null|  4|356|\n",
      "+------------+-----+-----+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe\n",
    "\n",
    "The first thing we'll do is use the `describe` method to get some basics.  Note that **describe will return a new dataframe** with the parameters, so we'll assign the results to a new variable and then call `show` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "|summary|                  C0|                 C3|                C4|                C5|               C6|\n",
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "|  count|             3526154|            3526154|           1580402|           3526154|          3526154|\n",
      "|   mean| 5.50388599500189E11|  4.178168090221902|234846.78065481808| 5.134865351881966|354.7084951479714|\n",
      "| stddev|2.596112361975222...|0.34382335723646484|118170.68592261616|3.3833930336063456|4.011812510792076|\n",
      "|    min|        100002091588|               2.75|              0.85|                -1|              292|\n",
      "|    max|        999995696635|              6.125|        1193544.39|                34|              480|\n",
      "+-------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_described = df.describe()\n",
    "df_described.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from the five included in `describe`, there are a handful of other built-in aggregators that can be applied to a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  skewness(C3,0,0)|\n",
      "+------------------+\n",
      "|0.5197993394959969|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis, var_pop, var_samp, stddev, stddev_pop, sumDistinct\n",
    "df.select(skewness('C3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding the Describe Output\n",
    "\n",
    "One convenient thing we might want to do is put all our summary statistics together in one spot - in essence, expand the output from `describe`.  Below I'll go into a short example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(C0='-0.00183847089857', C3='0.519799339496', C4='0.758411576756', C5='0.286480156084', C6='-2.69765201567', summary='skew'), Row(C0='-1.19900726351', C3='0.126057726847', C4='0.576085602656', C5='0.195187780089', C6='24.7237858944', summary='kurtosis')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "columns = df_described.columns  #a list of the column names: ['summary', 'C0', 'C3', 'C4', 'C5', 'C6']\n",
    "funcs   = [skewness, kurtosis]  #a list of the functions we want to add to our summary statistics (imported earlier)\n",
    "fnames  = ['skew', 'kurtosis']  #a list of strings describing the functions in the same order\n",
    "\n",
    "def new_item(func, column):\n",
    "    \"\"\"\n",
    "    This function takes in an aggregation function and a column name, then applies the aggregation to the column,\n",
    "    collects it and returns a value.  The value is in string format despite being a number, because that matches\n",
    "    the output of describe.\n",
    "    \"\"\"\n",
    "    return str(df.select(func(column)).collect()[0][0])\n",
    "\n",
    "new_data = []\n",
    "for func, fname in zip(funcs, fnames):\n",
    "    row_dict = {'summary':fname}  #each row object begins with an entry for \"summary\"\n",
    "    for column in columns[1:]:\n",
    "        row_dict[column] = new_item(func, column)\n",
    "    new_data.append(Row(**row_dict))  #the ** format tells Python to unpack the entries of the dictionary we just built\n",
    "    \n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates through the entries in `funcs` and `fnames` together, then builds a new row object following the format of the standard `describe` output.  You can see from the output that it looks nearly identical to the output of `collect` when applied to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(summary=u'count', C0=u'3526154', C3=u'3526154', C4=u'1580402', C5=u'3526154', C6=u'3526154'), Row(summary=u'mean', C0=u'5.50388599500189E11', C3=u'4.178168090221902', C4=u'234846.78065481808', C5=u'5.134865351881966', C6=u'354.7084951479714'), Row(summary=u'stddev', C0=u'2.5961123619752228E11', C3=u'0.34382335723646484', C4=u'118170.68592261616', C5=u'3.3833930336063456', C6=u'4.011812510792076'), Row(summary=u'min', C0=u'100002091588', C3=u'2.75', C4=u'0.85', C5=u'-1', C6=u'292'), Row(summary=u'max', C0=u'999995696635', C3=u'6.125', C4=u'1193544.39', C5=u'34', C6=u'480')]\n"
     ]
    }
   ],
   "source": [
    "desc_collect = df_described.collect()\n",
    "print(desc_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the columns are out of order within the rows; this is because we built them from a dictionary, and dictionary entries in Python are inherently unordered.  We will fix that below.\n",
    "\n",
    "The next step is to join the two sets of data into one, in order to make a modified `describe` output that includes skew and kurtosis.  The same method could be used to include any other aggregations desired.\n",
    "\n",
    "One side note: we perform the join by **first turning the `describe` output into a list of raw row objects using `collect` (as in the above codeblock), then turning it back into a dataframe.**  This is due to a bug in Spark 1.6 and earlier where `describe` output wasn't serializable.  It was raised on the Spark Jira board and fixed with a patch, however that patch is not deployed to Spark 1.6 on Amazon Web Services.  The problem is circumvented by breaking it down into the raw data and then building it back to a dataframe.  Once AWS upgrades to Spark 2.0 (or higher) one should obviously skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "| summary|                  C0|                 C3|                C4|                C5|               C6|\n",
      "+--------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "|   count|             3526154|            3526154|           1580402|           3526154|          3526154|\n",
      "|    mean| 5.50388599500189E11|  4.178168090221902|234846.78065481808| 5.134865351881966|354.7084951479714|\n",
      "|  stddev|2.596112361975222...|0.34382335723646484|118170.68592261616|3.3833930336063456|4.011812510792076|\n",
      "|     min|        100002091588|               2.75|              0.85|                -1|              292|\n",
      "|     max|        999995696635|              6.125|        1193544.39|                34|              480|\n",
      "|    skew|   -0.00183847089857|     0.519799339496|    0.758411576756|    0.286480156084|   -2.69765201567|\n",
      "|kurtosis|      -1.19900726351|     0.126057726847|    0.576085602656|    0.195187780089|    24.7237858944|\n",
      "+--------+--------------------+-------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd1 = sc.parallelize(new_data).toDF()     #turns the results from our loop into a dataframe\n",
    "dd2 = sc.parallelize(desc_collect).toDF() #turns the collected results from describe back into a dataframe\n",
    "dd1 = dd1.select(dd2.columns) #this forces the columns in our new_data to be in the same order as in describe\n",
    "\n",
    "expanded_describe = dd2.unionAll(dd1)\n",
    "expanded_describe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we have our expanded `describe` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

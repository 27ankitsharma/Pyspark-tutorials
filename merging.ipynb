{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Merging and Joining Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 17 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will go over the various ways to concatenate two or more dataframes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some basic setup, first to verify that the Spark Context was successfully created by the startup script and then to import the SQL structure that supports the dataframes we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have the same columns in each dataframe and just want to stack one on top of the other, row-wise.  We can make this happen with a helper function, after we first build three simple toy dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(\"name\", \"pet\", \"count\")\n",
    "\n",
    "df1 = sc.parallelize([\n",
    "    row(\"Sue\", \"cat\", 16),\n",
    "    row(\"Kim\", \"dog\", 1),    \n",
    "    row(\"Bob\", \"fish\", 5)\n",
    "    ]).toDF()\n",
    "\n",
    "df2 = sc.parallelize([\n",
    "    row(\"Fred\", \"cat\", 2),\n",
    "    row(\"Kate\", \"ant\", 179),    \n",
    "    row(\"Marc\", \"lizard\", 5)\n",
    "    ]).toDF()\n",
    "\n",
    "df3 = sc.parallelize([\n",
    "    row(\"Sarah\", \"shark\", 3),\n",
    "    row(\"Jason\", \"kids\", 2),    \n",
    "    row(\"Scott\", \"squirrel\", 1)\n",
    "    ]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "def union_many(*dfs):\n",
    "    #this function can have as many dataframes as you want passed into it\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df_union = union_many(df1, df2, df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| name|     pet|count|\n",
      "+-----+--------+-----+\n",
      "|  Sue|     cat|   16|\n",
      "|  Kim|     dog|    1|\n",
      "|  Bob|    fish|    5|\n",
      "| Fred|     cat|    2|\n",
      "| Kate|     ant|  179|\n",
      "| Marc|  lizard|    5|\n",
      "|Sarah|   shark|    3|\n",
      "|Jason|    kids|    2|\n",
      "|Scott|squirrel|    1|\n",
      "+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way to merge is by combining columns on certain keys across rows.  There are four ways to specify the logic of the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row1 = Row(\"name\", \"pet\", \"count\")\n",
    "row2 = Row(\"name\", \"pet2\", \"count2\")\n",
    "\n",
    "df1 = sc.parallelize([\n",
    "    row1(\"Sue\", \"cat\", 16),\n",
    "    row1(\"Kim\", \"dog\", 1),    \n",
    "    row1(\"Bob\", \"fish\", 5),\n",
    "    row1(\"Libuse\", \"horse\", 1)\n",
    "    ]).toDF()\n",
    "\n",
    "df2 = sc.parallelize([\n",
    "    row2(\"Sue\", \"eagle\", 2),\n",
    "    row2(\"Kim\", \"ant\", 179),    \n",
    "    row2(\"Bob\", \"lizard\", 5),\n",
    "    row2(\"Ferdinand\", \"bees\", 23)\n",
    "    ]).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll do an `inner` join, which merges rows that have a match in both dataframes and drops all others.  This is the default type of join, so the `how` argument could be omitted here if you didn't wish to be explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------+\n",
      "|name| pet|count|  pet2|count2|\n",
      "+----+----+-----+------+------+\n",
      "| Sue| cat|   16| eagle|     2|\n",
      "| Bob|fish|    5|lizard|     5|\n",
      "| Kim| dog|    1|   ant|   179|\n",
      "+----+----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, 'name', how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then an `outer` join, which uses all rows regardless of matches and fills in `null` for missing observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+------+------+\n",
      "|     name|  pet|count|  pet2|count2|\n",
      "+---------+-----+-----+------+------+\n",
      "|Ferdinand| null| null|  bees|    23|\n",
      "|      Sue|  cat|   16| eagle|     2|\n",
      "|      Bob| fish|    5|lizard|     5|\n",
      "|   Libuse|horse|    1|  null|  null|\n",
      "|      Kim|  dog|    1|   ant|   179|\n",
      "+---------+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, 'name', how='outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a `left` join uses all keys from the left dataframe (in this case `df1`) but only rows with matches from the right dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+------+------+\n",
      "|  name|  pet|count|  pet2|count2|\n",
      "+------+-----+-----+------+------+\n",
      "|   Sue|  cat|   16| eagle|     2|\n",
      "|   Bob| fish|    5|lizard|     5|\n",
      "|Libuse|horse|    1|  null|  null|\n",
      "|   Kim|  dog|    1|   ant|   179|\n",
      "+------+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, 'name', how='left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `right` join would just be the opposte of that.  That is equivalent to performing a `left` join but switching the places of `df1` and `df2` in the code block (although the resulting dataframes would have different column orderings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a finally, note that merges can be done on multiple columns by passing a **list** of column name strings instead of just a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Loading and Exploring CSV Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 15 June 2016, Spark v1.6.1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will go over loading a CSV file into a dataframe, setting data types, and renaming columns._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few initial setup items:  First we test that the spark context was successfully created during bootstrap and is available in the global namespace as 'sc'.  After that we create the SQL context necessary for working with a dataframe (panel data).  Note that the spark-csv package we'll be using is installed automatically in the bootstrap script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    raise Exception('Spark context not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load our data from a CSV file in an S3 bucket.  There are three ways to handle data types (dtypes) for each column: The easiest, but the most computationally-expensive, is to pass `inferSchema='true'` to the load call.  The second way entails specifiying the dtypes manually by passing `schema=StructType(...)` to the load call, which is computationally-efficient but may be difficult and prone to coder error for especially wide datasets.  The final option is to not specify a schema, in which case all the columns will have string dtypes.  Note that dtypes can be changed later, though it is more costly than doing it correctly in the loading process.\n",
    "\n",
    "Loading the data with the schema inferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          inferSchema='true',\n",
    "                          delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example loading of the same data by passing a custom schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType, FloatType, LongType, DoubleType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "customSchema = StructType([StructField('C0', DateType(), True),\n",
    "                           StructField('C1', StringType(), True),\n",
    "                           StructField('C2', DoubleType(), True),\n",
    "                           StructField('C3', DoubleType(), True),\n",
    "                           StructField('C4', DoubleType(), True),\n",
    "                           StructField('C5', IntegerType(), True),\n",
    "                           ...\n",
    "                           StructField('C27', StringType(), True)])\n",
    "                           \n",
    "df = sqlContext.read.load('s3://ui-hfpc/Performance_2015Q1.txt',\n",
    "                          format='com.databricks.spark.csv',\n",
    "                          header='false',\n",
    "                          schema='customSchema',\n",
    "                          delimiter='|')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526154"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see what the dtypes are for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0', 'bigint'),\n",
       " ('C1', 'string'),\n",
       " ('C2', 'string'),\n",
       " ('C3', 'double'),\n",
       " ('C4', 'double'),\n",
       " ('C5', 'int'),\n",
       " ('C6', 'int'),\n",
       " ('C7', 'int'),\n",
       " ('C8', 'string'),\n",
       " ('C9', 'int'),\n",
       " ('C10', 'string'),\n",
       " ('C11', 'string'),\n",
       " ('C12', 'int'),\n",
       " ('C13', 'string'),\n",
       " ('C14', 'string'),\n",
       " ('C15', 'string'),\n",
       " ('C16', 'string'),\n",
       " ('C17', 'string'),\n",
       " ('C18', 'string'),\n",
       " ('C19', 'string'),\n",
       " ('C20', 'string'),\n",
       " ('C21', 'string'),\n",
       " ('C22', 'string'),\n",
       " ('C23', 'string'),\n",
       " ('C24', 'string'),\n",
       " ('C25', 'string'),\n",
       " ('C26', 'int'),\n",
       " ('C27', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peak at five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(C0=100002091588, C1=u'01/01/2015', C2=u'OTHER', C3=4.125, C4=None, C5=0, C6=360, C7=360, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'02/01/2015', C2=u'', C3=4.125, C4=None, C5=1, C6=359, C7=359, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'03/01/2015', C2=u'', C3=4.125, C4=None, C5=2, C6=358, C7=358, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'04/01/2015', C2=u'', C3=4.125, C4=None, C5=3, C6=357, C7=357, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u''),\n",
       " Row(C0=100002091588, C1=u'05/01/2015', C2=u'', C3=4.125, C4=None, C5=4, C6=356, C7=356, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u'')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data came with no headers, so pySpark named all columns *C0, C1, C2, ..., Cn*.  We can rename columns one or a few at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('C0','id').withColumnRenamed('C1','date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=100002091588, date=u'01/01/2015', C2=u'OTHER', C3=4.125, C4=None, C5=0, C6=360, C7=360, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u'')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Or rename many of them in a loop using two lists or a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_names = ['C2', 'C3', 'C4', 'C5', 'C6', 'C7']\n",
    "new_names = ['foo', 'bar', 'baz', 'more', 'another', 'stuff']\n",
    "for old, new in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=100002091588, date=u'01/01/2015', foo=u'OTHER', bar=4.125, baz=None, more=0, another=360, stuff=360, C8=u'01/2045', C9=16740, C10=u'0', C11=u'N', C12=None, C13=u'', C14=u'', C15=u'', C16=u'', C17=u'', C18=u'', C19=u'', C20=u'', C21=u'', C22=u'', C23=u'', C24=u'', C25=u'', C26=None, C27=u'')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'date',\n",
       " 'foo',\n",
       " 'bar',\n",
       " 'baz',\n",
       " 'more',\n",
       " 'another',\n",
       " 'stuff',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'C22',\n",
       " 'C23',\n",
       " 'C24',\n",
       " 'C25',\n",
       " 'C26',\n",
       " 'C27']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll describe the data.  Note that `describe` returns a new dataframe with the information, and so must have `show` called after it if our goal is to view it.  This can be called on one or more specific columns, as we do here, or the entire dataframe by passing no columns to describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+\n",
      "|summary|                 foo|                bar|               baz|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "|  count|             3526154|            3526154|           1580402|\n",
      "|   mean|                null|  4.178168090221902|234846.78065481802|\n",
      "| stddev|                null|0.34382335723646484|118170.68592261615|\n",
      "|    min|                    |               2.75|              0.85|\n",
      "|    max|WELLS FARGO BANK,...|              6.125|        1193544.39|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('foo', 'bar', 'baz').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with big data can be difficult, because we're often used to being able to \"look\" at all of our data in Excel to get a feel for it.  Dealing with data so large that it can't be loaded into memory is entirely doable but requires a shift in thinking.  When necessary, tiny subsets can be used as with the `take` command used repeatedly above, though doing so can be an expensive operation.\n",
    "\n",
    "HOWEVER, the ability to look at your data in a traditional way does exist in pySpark; you just can't do it unless the data fits in the memory of one computer.  With that crucial caveat in mind, here are some commands you can use on a pySpark dataframe, but usually shouldn't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df.show()                          #show the entire dataframe\n",
    "df.select('column_name').show()    #show the entire selected row\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

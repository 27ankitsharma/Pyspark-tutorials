{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_pySpark Basics: Loading, Exploring and Saving Data_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Jeff Levy (jlevy@urban.org)_\n",
    "\n",
    "_Last Updated: 8 Aug 2016, Spark v2.0_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Abstract: This guide will go over loading a CSV file into a dataframe, exploring it with basic commands, and finally writing it out to file in S3 and reading it back in._\n",
    "\n",
    "_Main operations used:_ `write.save`, `read.load`, `count`, `dtypes`, `schema`/`inferSchema`, `take`, `show`, `withColumnRenamed`, `columns`, `describe`, `coalesce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial setup item that you will see in all of the Spark 1.6.1 tutorials: We create the SQL context necessary for working with a dataframe (panel data).  Note that the spark-csv package we'll be using is installed automatically in the bootstrap script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading From CSV\n",
    "\n",
    "Next we load our data from a CSV file in an S3 bucket.  There are three ways to handle data types (dtypes) for each column: The easiest, but the most computationally-expensive, is to pass `inferSchema=True` to the load method.  The second way entails specifiying the dtypes manually for every column by passing `schema=StructType(...)`, which is computationally-efficient but may be difficult and prone to coder error for especially wide datasets.  The final option is to not specify a schema option at all, in which case Spark will assign all the columns string dtypes.  Note that dtypes can be changed later, as we will demonstrate, though it is more costly than doing it correctly in the loading process.\n",
    "\n",
    "Loading the data with the schema inferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('s3://ui-spark-data/Performance_2015Q1.txt', header=False, inferSchema=True, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example loading of the same data by passing a custom schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType, FloatType, LongType, DoubleType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "customSchema = StructType([StructField('_c0', DateType(), True),\n",
    "                           StructField('_c1', StringType(), True),\n",
    "                           StructField('_c2', DoubleType(), True),\n",
    "                           StructField('_c3', DoubleType(), True),\n",
    "                           StructField('_c4', DoubleType(), True),\n",
    "                           StructField('_c5', IntegerType(), True),\n",
    "                           ...\n",
    "                           StructField('_c27', StringType(), True)])\n",
    "                           \n",
    "df = spark.read.csv('s3://ui-spark-data/Performance_2015Q1.txt', header=False, schema=customSchema, sep='|')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of using infering and specifying a schema together might be with a large, unfamiliar dataset that you know you will need to load up and work with repeatedly.  The first time you load it use `inferSchema`, then make note of the dtypes it assigns.  Use that information to build the custom schema, so that when you load the data in the future you avoid the extra processing time necessary for infering.\n",
    "\n",
    "# Exploring the Data\n",
    "\n",
    "Our data is now loaded into a dataframe that we named `df`, with all the dtypes inferred.  First we'll count the number of rows it found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526154"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look at the column-by-column dtypes the system estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'bigint'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'double'),\n",
       " ('_c4', 'double'),\n",
       " ('_c5', 'int'),\n",
       " ('_c6', 'int'),\n",
       " ('_c7', 'int'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'int'),\n",
       " ('_c10', 'string'),\n",
       " ('_c11', 'string'),\n",
       " ('_c12', 'int'),\n",
       " ('_c13', 'string'),\n",
       " ('_c14', 'string'),\n",
       " ('_c15', 'string'),\n",
       " ('_c16', 'string'),\n",
       " ('_c17', 'string'),\n",
       " ('_c18', 'string'),\n",
       " ('_c19', 'string'),\n",
       " ('_c20', 'string'),\n",
       " ('_c21', 'string'),\n",
       " ('_c22', 'string'),\n",
       " ('_c23', 'string'),\n",
       " ('_c24', 'string'),\n",
       " ('_c25', 'string'),\n",
       " ('_c26', 'int'),\n",
       " ('_c27', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pairing (a `tuple` object in Python, denoted by the parentheses), the first entry is the column name and the second is the dtype.  Notice that this data has no headers with it (we specified `headers=False` when we loaded it), so Spark used its default naming convention of `_c0, _c1, ... _cn`.  We'll makes some changes to that in a minute.\n",
    "\n",
    "Take a peak at five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=100002091588, _c1=u'01/01/2015', _c2=u'OTHER', _c3=4.125, _c4=None, _c5=0, _c6=360, _c7=360, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u''),\n",
       " Row(_c0=100002091588, _c1=u'02/01/2015', _c2=u'', _c3=4.125, _c4=None, _c5=1, _c6=359, _c7=359, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u''),\n",
       " Row(_c0=100002091588, _c1=u'03/01/2015', _c2=u'', _c3=4.125, _c4=None, _c5=2, _c6=358, _c7=358, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u''),\n",
       " Row(_c0=100002091588, _c1=u'04/01/2015', _c2=u'', _c3=4.125, _c4=None, _c5=3, _c6=357, _c7=357, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u''),\n",
       " Row(_c0=100002091588, _c1=u'05/01/2015', _c2=u'', _c3=4.125, _c4=None, _c5=4, _c6=356, _c7=356, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u'')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the format `column_name=value` for each row.  Note that the formatting above is ugly because `take` doesn't try to make it pretty, it just returns the row object itself.  We can use `show` instead and that attempts to format the data better, but because there are so many columns in this case the formatting of `show` doesn't fit, and each line wraps down to the next.  We'll use `show` on a subset below.\n",
    "\n",
    "# Renaming Columns\n",
    "\n",
    "We can rename columns one at a time, or several at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_c0','id').withColumnRenamed('_c1','date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=100002091588, date=u'01/01/2015', _c2=u'OTHER', _c3=4.125, _c4=None, _c5=0, _c6=360, _c7=360, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u'')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see that column `C0` has been renamed to `id`, and `C1` to `date`.\n",
    "\n",
    "We can also rename many of them in a loop using two lists or a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_names = ['_c2', '_c3', '_c4', '_c5', '_c6', '_c7']\n",
    "new_names = ['foo', 'bar', 'baz', 'more', 'another', 'stuff']\n",
    "for old, new in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=100002091588, date=u'01/01/2015', foo=u'OTHER', bar=4.125, baz=None, more=0, another=360, stuff=360, _c8=u'01/2045', _c9=16740, _c10=u'0', _c11=u'N', _c12=None, _c13=u'', _c14=u'', _c15=u'', _c16=u'', _c17=u'', _c18=u'', _c19=u'', _c20=u'', _c21=u'', _c22=u'', _c23=u'', _c24=u'', _c25=u'', _c26=None, _c27=u'')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'date',\n",
       " 'foo',\n",
       " 'bar',\n",
       " 'baz',\n",
       " 'more',\n",
       " 'another',\n",
       " 'stuff',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13',\n",
       " '_c14',\n",
       " '_c15',\n",
       " '_c16',\n",
       " '_c17',\n",
       " '_c18',\n",
       " '_c19',\n",
       " '_c20',\n",
       " '_c21',\n",
       " '_c22',\n",
       " '_c23',\n",
       " '_c24',\n",
       " '_c25',\n",
       " '_c26',\n",
       " '_c27']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe\n",
    "\n",
    "Now we'll describe the data.  Note that `describe` returns a new dataframe with the information, and so must have `show` called after it if our goal is to view it (note the nice formatting in this case).  This can be called on one or more specific columns, as we do here, or the entire dataframe by passing no columns to describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+\n",
      "|summary|                 foo|                bar|               baz|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "|  count|             3526154|            3526154|           1580402|\n",
      "|   mean|                null|  4.178168090219519|234846.78065481762|\n",
      "| stddev|                null|0.34382335723646673|118170.68592261661|\n",
      "|    min|                    |               2.75|              0.85|\n",
      "|    max|WELLS FARGO BANK,...|              6.125|        1193544.39|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_described = df.describe('foo', 'bar', 'baz')\n",
    "df_described.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Writing to S3\n",
    "\n",
    "And finally, we can write data out to our S3 bucket.  Note that if your data is small enough to be collected onto one computer, writing it is easy.  We'll use the dataframe we just created using `describe` above as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_described.write.csv('s3://pyspark-tutorials/mycsv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above line will turn *each partition* of this dataframe into a .csv file.  This is an important note; if your data is very big it may be on a lot of partitions.  This may be required if your data too large to fit in one csv file, but if your data should fit you can include the `coalesce` command, like this:\n",
    "\n",
    "    df_described.coalesce(1).write.csv('s3://pyspark-tutorials/mycsv', header=True)\n",
    "\n",
    "To tell it to combine all the data into 1 partition (or however many you pass in as the value).  Again, only do this if your data isn't very large.  See the pySpark tutorial on subsetting for more.\n",
    "\n",
    "Now you can read our output back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = spark.read.csv('s3://pyspark-tutorials/mycsv', header=True, inferSchema=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------------+\n",
      "|summary|                 foo|                bar|               baz|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "| stddev|                    |0.34382335723646673|118170.68592261661|\n",
      "|    min|                    |               2.75|              0.85|\n",
      "|    max|WELLS FARGO BANK,...|              6.125|        1193544.39|\n",
      "|  count|             3526154|          3526154.0|         1580402.0|\n",
      "|   mean|                    |  4.178168090219519|234846.78065481762|\n",
      "+-------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the differences from the original loading we did at the top of this tutorial; we pointed it to the new path we created on S3, we told it our data had headers that we wanted to keep, and the delimiter is now the default `,` instead of the `|` the original data had - we could have specified this as an option in our write operation if we had wanted it to be something else.  Also, since commas are the default we could have left the `delimiter` argument out all together here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
